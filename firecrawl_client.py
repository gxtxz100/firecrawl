#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Firecrawl å‹å¥½å®¢æˆ·ç«¯
===================

è¿™æ˜¯ä¸€ä¸ªå¯¹ Firecrawl API çš„å‹å¥½åŒ…è£…ï¼Œè®©æ‚¨èƒ½å¤Ÿæ›´è½»æ¾åœ°ä½¿ç”¨ Firecrawl è¿›è¡Œç½‘é¡µæŠ“å–å’Œçˆ¬å–ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- ç®€æ´æ˜“ç”¨çš„ API
- å‹å¥½çš„ä¸­æ–‡é”™è¯¯æç¤º
- è‡ªåŠ¨å¤„ç†å¸¸è§é”™è¯¯
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
- å†…ç½®è¿›åº¦æ˜¾ç¤º

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from firecrawl_client import FirecrawlClient
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = FirecrawlClient(api_key="your-api-key")
    
    # æŠ“å–å•ä¸ªç½‘é¡µ
    result = client.æŠ“å–ç½‘é¡µ("https://example.com")
    print(result.å†…å®¹)
    
    # çˆ¬å–æ•´ä¸ªç½‘ç«™
    results = client.çˆ¬å–ç½‘ç«™("https://example.com", æœ€å¤§é¡µé¢æ•°=10)
    for page in results:
        print(page.æ ‡é¢˜)
"""

import os
import sys
import re
from typing import Optional, List, Dict, Any, Union
from datetime import datetime

# å°è¯•å¯¼å…¥ DuckDuckGo æœç´¢åº“ï¼ˆå¯é€‰ï¼Œç”¨äºå…è´¹æœç´¢ï¼‰
# æ–°ç‰ˆæœ¬ä½¿ç”¨ ddgsï¼Œæ—§ç‰ˆæœ¬ä½¿ç”¨ duckduckgo_search
DDGS = None
DDG_AVAILABLE = False
try:
    # ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬ ddgs
    from ddgs import DDGS
    DDG_AVAILABLE = True
except ImportError:
    try:
        # å›é€€åˆ°æ—§ç‰ˆæœ¬ duckduckgo_search
        from duckduckgo_search import DDGS
        DDG_AVAILABLE = True
    except ImportError:
        DDG_AVAILABLE = False

# å°è¯•å¯¼å…¥æœ¬åœ°æ–‡ç« æå–åº“ï¼ˆå¯é€‰ï¼Œç”¨äºå…è´¹æå–ï¼‰
try:
    from readability.readability import Document as ReadabilityDocument
    from bs4 import BeautifulSoup
    import html2text
    from dateutil import parser as date_parser
    import requests
    LOCAL_EXTRACT_AVAILABLE = True
except ImportError:
    try:
        # å°è¯•å¦ä¸€ç§å¯¼å…¥æ–¹å¼
        from readability import Document as ReadabilityDocument
        from bs4 import BeautifulSoup
        import html2text
        from dateutil import parser as date_parser
        import requests
        LOCAL_EXTRACT_AVAILABLE = True
    except ImportError:
        LOCAL_EXTRACT_AVAILABLE = False

try:
    from firecrawl import Firecrawl
    from firecrawl.v2.types import Document, CrawlJob, ScrapeOptions
except ImportError:
    try:
        # å°è¯•ä»æœ¬åœ°è·¯å¾„å¯¼å…¥ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sdk_path = os.path.join(current_dir, 'apps', 'python-sdk')
        if os.path.exists(sdk_path) and sdk_path not in sys.path:
            sys.path.insert(0, sdk_path)
        from firecrawl import Firecrawl
        from firecrawl.v2.types import Document, CrawlJob, ScrapeOptions
    except ImportError as e:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ° firecrawl åº“ã€‚")
        print("è¯·ç¡®ä¿ï¼š")
        print("1. å·²å®‰è£… firecrawl-py: pip install firecrawl-py")
        print("2. æˆ–å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source venv/bin/activate")
        print("3. æˆ–åœ¨ firecrawl é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        print(f"\nè¯¦ç»†é”™è¯¯: {e}")
        sys.exit(1)


class FirecrawlClient:
    """
    Firecrawl å‹å¥½å®¢æˆ·ç«¯
    
    è¿™ä¸ªç±»æä¾›äº†å¯¹ Firecrawl API çš„å‹å¥½åŒ…è£…ï¼Œè®©æ‚¨èƒ½å¤Ÿæ›´è½»æ¾åœ°è¿›è¡Œç½‘é¡µæŠ“å–ã€‚
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: str = "https://api.firecrawl.dev",
        timeout: Optional[float] = None,
        max_retries: int = 3
    ):
        """
        åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
        
        å‚æ•°:
            api_key: Firecrawl API å¯†é’¥ã€‚å¦‚æœä¸æä¾›ï¼Œå°†ä»ç¯å¢ƒå˜é‡ FIRECRAWL_API_KEY è¯»å–
            api_url: API æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤ä¸ºå®˜æ–¹äº‘æœåŠ¡ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        ç¤ºä¾‹:
            # ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥
            client = FirecrawlClient()
            
            # ç›´æ¥æä¾› API å¯†é’¥
            client = FirecrawlClient(api_key="fc-your-api-key")
        """
        # å¦‚æœæ²¡æœ‰æä¾› API å¯†é’¥ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        if api_key is None:
            api_key = os.getenv("FIRECRAWL_API_KEY")
        
        self.api_key = api_key
        self.timeout = timeout
        self.max_retries = max_retries
        
        # å¦‚æœæ²¡æœ‰ API å¯†é’¥ï¼Œåªåˆå§‹åŒ–ç”¨äºå…è´¹æœç´¢ï¼ˆä¸åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯ï¼‰
        if not api_key:
            self._client = None
            print(f"ğŸ’¡ æœªè®¾ç½® API å¯†é’¥ï¼Œä»…å¯ä½¿ç”¨å…è´¹æœç´¢åŠŸèƒ½")
            print(f"   å¦‚éœ€ä½¿ç”¨å®Œæ•´åŠŸèƒ½ï¼Œè¯·è®¾ç½® API å¯†é’¥ï¼šhttps://firecrawl.dev")
            return
        
        # æœ‰ API å¯†é’¥æ—¶ï¼Œåˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
        try:
            # Firecrawl åªæ¥å— api_key å’Œ api_url å‚æ•°
            self._client = Firecrawl(
                api_key=api_key,
                api_url=api_url
            )
            print(f"âœ“ Firecrawl å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
    
    def æŠ“å–ç½‘é¡µ(
        self,
        url: str,
        æ ¼å¼: Optional[List[str]] = None,
        ä»…ä¸»è¦å†…å®¹: bool = True,
        ç­‰å¾…æ—¶é—´: Optional[int] = None,
        ç§»åŠ¨ç«¯: bool = False,
        **kwargs
    ) -> 'ç½‘é¡µç»“æœ':
        """
        æŠ“å–å•ä¸ªç½‘é¡µ
        
        å‚æ•°:
            url: è¦æŠ“å–çš„ç½‘é¡µ URL
            æ ¼å¼: è¾“å‡ºæ ¼å¼åˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼š['markdown', 'html', 'links', 'screenshot']
            ä»…ä¸»è¦å†…å®¹: æ˜¯å¦åªæŠ“å–ä¸»è¦å†…å®¹ï¼ˆå»é™¤å¯¼èˆªæ ã€é¡µè„šç­‰ï¼‰
            ç­‰å¾…æ—¶é—´: ç­‰å¾…é¡µé¢åŠ è½½çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            ç§»åŠ¨ç«¯: æ˜¯å¦ä½¿ç”¨ç§»åŠ¨ç«¯è§†å›¾
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            ç½‘é¡µç»“æœå¯¹è±¡ï¼ŒåŒ…å«å†…å®¹ã€å…ƒæ•°æ®ç­‰ä¿¡æ¯
        
        ç¤ºä¾‹:
            result = client.æŠ“å–ç½‘é¡µ("https://example.com")
            print(result.å†…å®¹)  # Markdown æ ¼å¼çš„å†…å®¹
            print(result.æ ‡é¢˜)  # ç½‘é¡µæ ‡é¢˜
            print(result.å…ƒæ•°æ®)  # å®Œæ•´çš„å…ƒæ•°æ®å­—å…¸
        """
        if æ ¼å¼ is None:
            æ ¼å¼ = ["markdown"]
        
        if not self._client:
            raise ValueError("æ­¤åŠŸèƒ½éœ€è¦ API å¯†é’¥ã€‚è¯·è®¾ç½® FIRECRAWL_API_KEY æˆ–ä½¿ç”¨å…è´¹æœç´¢åŠŸèƒ½ã€‚")
        
        try:
            print(f"æ­£åœ¨æŠ“å–: {url}")
            document = self._client.scrape(
                url=url,
                formats=æ ¼å¼,
                only_main_content=ä»…ä¸»è¦å†…å®¹,
                wait_for=ç­‰å¾…æ—¶é—´,
                mobile=ç§»åŠ¨ç«¯,
                **kwargs
            )
            print(f"âœ“ æŠ“å–æˆåŠŸ")
            return ç½‘é¡µç»“æœ(document)
        except Exception as e:
            error_msg = str(e)
            if "401" in error_msg or "unauthorized" in error_msg.lower():
                raise ValueError("API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥æ˜¯å¦æ­£ç¡®")
            elif "404" in error_msg:
                raise ValueError(f"æ— æ³•è®¿é—®è¯¥ URL: {url}\nè¯·æ£€æŸ¥ URL æ˜¯å¦æ­£ç¡®")
            elif "timeout" in error_msg.lower():
                raise TimeoutError(f"è¯·æ±‚è¶…æ—¶: {url}\nè¯·å°è¯•å¢åŠ ç­‰å¾…æ—¶é—´æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
            else:
                raise RuntimeError(f"æŠ“å–å¤±è´¥: {error_msg}")
    
    def çˆ¬å–ç½‘ç«™(
        self,
        url: str,
        æœ€å¤§é¡µé¢æ•°: Optional[int] = 10,
        æ ¼å¼: Optional[List[str]] = None,
        æ’é™¤è·¯å¾„: Optional[List[str]] = None,
        åŒ…å«è·¯å¾„: Optional[List[str]] = None,
        è½®è¯¢é—´éš”: int = 2,
        è¶…æ—¶: Optional[int] = None,
        **kwargs
    ) -> List['ç½‘é¡µç»“æœ']:
        """
        çˆ¬å–æ•´ä¸ªç½‘ç«™
        
        å‚æ•°:
            url: èµ·å§‹ URL
            æœ€å¤§é¡µé¢æ•°: æœ€å¤šçˆ¬å–çš„é¡µé¢æ•°é‡
            æ ¼å¼: è¾“å‡ºæ ¼å¼åˆ—è¡¨
            æ’é™¤è·¯å¾„: è¦æ’é™¤çš„ URL è·¯å¾„æ¨¡å¼åˆ—è¡¨
            åŒ…å«è·¯å¾„: è¦åŒ…å«çš„ URL è·¯å¾„æ¨¡å¼åˆ—è¡¨
            è½®è¯¢é—´éš”: æ£€æŸ¥çˆ¬å–çŠ¶æ€çš„é—´éš”ï¼ˆç§’ï¼‰
            è¶…æ—¶: æ•´ä¸ªçˆ¬å–ä»»åŠ¡çš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            ç½‘é¡µç»“æœå¯¹è±¡åˆ—è¡¨
        
        ç¤ºä¾‹:
            results = client.çˆ¬å–ç½‘ç«™(
                "https://example.com",
                æœ€å¤§é¡µé¢æ•°=20,
                æ’é™¤è·¯å¾„=["/admin/*", "/private/*"]
            )
            for result in results:
                print(f"{result.æ ‡é¢˜}: {result.URL}")
        """
        if æ ¼å¼ is None:
            æ ¼å¼ = ["markdown"]
        
        if not self._client:
            raise ValueError("æ­¤åŠŸèƒ½éœ€è¦ API å¯†é’¥ã€‚è¯·è®¾ç½® FIRECRAWL_API_KEYã€‚")
        
        try:
            print(f"å¼€å§‹çˆ¬å–ç½‘ç«™: {url}")
            print(f"æœ€å¤§é¡µé¢æ•°: {æœ€å¤§é¡µé¢æ•°}")
            
            scrape_options = {
                "formats": æ ¼å¼,
                "only_main_content": True
            }
            
            crawl_job = self._client.crawl(
                url=url,
                limit=æœ€å¤§é¡µé¢æ•°,
                exclude_paths=æ’é™¤è·¯å¾„,
                include_paths=åŒ…å«è·¯å¾„,
                scrape_options=scrape_options,
                poll_interval=è½®è¯¢é—´éš”,
                timeout=è¶…æ—¶,
                **kwargs
            )
            
            print(f"âœ“ çˆ¬å–å®Œæˆ: å…± {crawl_job.completed} ä¸ªé¡µé¢")
            
            # è½¬æ¢ä¸ºå‹å¥½çš„ç»“æœåˆ—è¡¨
            results = []
            if hasattr(crawl_job, 'data') and crawl_job.data:
                for doc in crawl_job.data:
                    results.append(ç½‘é¡µç»“æœ(doc))
            
            return results
        except Exception as e:
            error_msg = str(e)
            if "401" in error_msg or "unauthorized" in error_msg.lower():
                raise ValueError("API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥æ˜¯å¦æ­£ç¡®")
            elif "timeout" in error_msg.lower():
                raise TimeoutError(f"çˆ¬å–è¶…æ—¶: {url}\nè¯·å°è¯•å¢åŠ è¶…æ—¶æ—¶é—´æˆ–å‡å°‘æœ€å¤§é¡µé¢æ•°")
            else:
                raise RuntimeError(f"çˆ¬å–å¤±è´¥: {error_msg}")
    
    def æœç´¢ç½‘é¡µ(
        self,
        æŸ¥è¯¢: str,
        ç»“æœæ•°é‡: int = 5,
        æŠ“å–å†…å®¹: bool = False,
        ä½¿ç”¨å…è´¹æœç´¢: bool = False,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç½‘é¡µ
        
        å‚æ•°:
            æŸ¥è¯¢: æœç´¢å…³é”®è¯
            ç»“æœæ•°é‡: è¿”å›çš„ç»“æœæ•°é‡
            æŠ“å–å†…å®¹: æ˜¯å¦æŠ“å–æœç´¢ç»“æœçš„å†…å®¹ï¼ˆä»… Firecrawl APIï¼‰
            ä½¿ç”¨å…è´¹æœç´¢: å¦‚æœä¸º Trueï¼Œä½¿ç”¨ DuckDuckGo å…è´¹æœç´¢ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« URLã€æ ‡é¢˜ã€æè¿°ç­‰ä¿¡æ¯
        
        ç¤ºä¾‹:
            # ä½¿ç”¨ Firecrawl API æœç´¢ï¼ˆéœ€è¦ API å¯†é’¥ï¼‰
            results = client.æœç´¢ç½‘é¡µ("Python æ•™ç¨‹", ç»“æœæ•°é‡=10)
            
            # ä½¿ç”¨å…è´¹æœç´¢ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
            results = client.æœç´¢ç½‘é¡µ("Python æ•™ç¨‹", ç»“æœæ•°é‡=10, ä½¿ç”¨å…è´¹æœç´¢=True)
        """
        # å¦‚æœä½¿ç”¨å…è´¹æœç´¢ï¼Œä½¿ç”¨ DuckDuckGo
        if ä½¿ç”¨å…è´¹æœç´¢:
            return self._å…è´¹æœç´¢(æŸ¥è¯¢, ç»“æœæ•°é‡)
        
        # å¦åˆ™ä½¿ç”¨ Firecrawl API
        if not self._client:
            raise ValueError("Firecrawl API æœç´¢éœ€è¦ API å¯†é’¥ã€‚è¯·è®¾ç½® FIRECRAWL_API_KEY æˆ–ä½¿ç”¨ ä½¿ç”¨å…è´¹æœç´¢=Trueã€‚")
        
        try:
            print(f"æ­£åœ¨æœç´¢: {æŸ¥è¯¢}")
            
            scrape_options = None
            if æŠ“å–å†…å®¹:
                scrape_options = {"formats": ["markdown"]}
            
            search_data = self._client.search(
                query=æŸ¥è¯¢,
                limit=ç»“æœæ•°é‡,
                scrape_options=scrape_options,
                **kwargs
            )
            
            print(f"âœ“ æ‰¾åˆ° {len(getattr(search_data, 'web', []) or [])} ä¸ªç»“æœ")
            
            # è½¬æ¢ä¸ºå‹å¥½çš„ç»“æœæ ¼å¼
            results = []
            web_results = getattr(search_data, 'web', []) or []
            for item in web_results:
                result = {
                    'url': getattr(item, 'url', ''),
                    'title': getattr(item, 'title', ''),
                    'description': getattr(item, 'description', ''),
                }
                # å¦‚æœæœ‰æŠ“å–çš„å†…å®¹
                if hasattr(item, 'markdown'):
                    result['content'] = item.markdown
                results.append(result)
            
            return results
        except Exception as e:
            error_msg = str(e)
            raise RuntimeError(f"æœç´¢å¤±è´¥: {error_msg}")
    
    def _åŒ…å«ä¸­æ–‡(self, æ–‡æœ¬: str) -> bool:
        if not æ–‡æœ¬:
            return False
        return any('\u4e00' <= ch <= '\u9fff' for ch in æ–‡æœ¬)
    
    def _ç»“æœç›¸å…³(self, æŸ¥è¯¢: str, result: Dict[str, Any]) -> bool:
        if not æŸ¥è¯¢:
            return True
        title = result.get('title') or ''
        description = result.get('description') or ''
        combined = f"{title} {description}".lower()
        url = (result.get('url') or '').lower()
        has_chinese = self._åŒ…å«ä¸­æ–‡(æŸ¥è¯¢)
        
        tokens = [token.lower() for token in re.split(r'\s+', æŸ¥è¯¢) if token.strip()]
        if not tokens:
            tokens = [æŸ¥è¯¢.lower()]
        
        for token in tokens:
            if token and token in combined:
                return True
            if token and token in url:
                return True
        
        if has_chinese:
            joined = æŸ¥è¯¢.replace(' ', '')
            if joined and joined in title + description:
                return True
        
        return False
    
    def _è¿‡æ»¤æœç´¢ç»“æœ(self, æŸ¥è¯¢: str, åŸå§‹ç»“æœ: List[Dict[str, Any]], ç›®æ ‡æ•°é‡: int) -> List[Dict[str, Any]]:
        if not åŸå§‹ç»“æœ:
            return []
        
        filtered: List[Dict[str, Any]] = []
        fallback: List[Dict[str, Any]] = []
        seen_urls = set()
        
        for item in åŸå§‹ç»“æœ:
            url = item.get('url')
            if not url or url in seen_urls:
                continue
            seen_urls.add(url)
            
            if self._ç»“æœç›¸å…³(æŸ¥è¯¢, item):
                filtered.append(item)
            else:
                fallback.append(item)
        
        if len(filtered) < ç›®æ ‡æ•°é‡:
            needed = ç›®æ ‡æ•°é‡ - len(filtered)
            filtered.extend(fallback[:needed])
        
        return filtered[:ç›®æ ‡æ•°é‡]
    
    def _æ¸…ç†æ–‡æœ¬(self, æ–‡æœ¬: str) -> str:
        if not æ–‡æœ¬:
            return ""
        cleaned = æ–‡æœ¬.replace("\u00a0", " ")
        cleaned = re.sub(r"\r\n?", "\n", cleaned)
        cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)
        cleaned = re.sub(r"[ \t]{2,}", " ", cleaned)
        return cleaned.strip()
    
    def _å…è´¹æœç´¢(
        self,
        æŸ¥è¯¢: str,
        ç»“æœæ•°é‡: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ DuckDuckGo è¿›è¡Œå…è´¹æœç´¢ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
        
        å‚æ•°:
            æŸ¥è¯¢: æœç´¢å…³é”®è¯
            ç»“æœæ•°é‡: è¿”å›çš„ç»“æœæ•°é‡
        
        è¿”å›:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not DDG_AVAILABLE:
            error_msg = (
                "DuckDuckGo æœç´¢åº“æœªå®‰è£…ã€‚\n"
                "è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                "  pip install ddgs\n"
                "  æˆ–è€…ï¼ˆæ—§ç‰ˆæœ¬ï¼‰: pip install duckduckgo-search\n"
                "\n"
                "æˆ–è€…åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ï¼š\n"
                "  source venv/bin/activate\n"
                "  pip install ddgs\n"
                "\n"
                "æˆ–è€…å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š\n"
                "  pip install -r requirements.txt"
            )
            raise RuntimeError(error_msg)
        
        try:
            print(f"æ­£åœ¨ä½¿ç”¨ DuckDuckGo å…è´¹æœç´¢: {æŸ¥è¯¢}")
            print("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯å…è´¹æœç´¢ï¼Œä¸éœ€è¦ API å¯†é’¥")
            
            raw_results: List[Dict[str, Any]] = []
            max_retries = 3
            retry_count = 0
            has_chinese = self._åŒ…å«ä¸­æ–‡(æŸ¥è¯¢)
            region = 'cn-zh' if has_chinese else 'wt-wt'
            fetch_limit = max(ç»“æœæ•°é‡ * 3, 15)
            
            while retry_count < max_retries:
                try:
                    with DDGS() as ddgs:
                        count = 0
                        search_iter = ddgs.text(
                            æŸ¥è¯¢,
                            max_results=fetch_limit,
                            region=region,
                            safesearch='Off'
                        )
                        
                        for r in search_iter:
                            if r is None:
                                continue
                            
                            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
                            url = r.get('href') or r.get('url') or ''
                            title = r.get('title') or r.get('text') or ''
                            description = r.get('body') or r.get('description') or r.get('snippet') or ''
                            
                            # åªæ·»åŠ æœ‰æ•ˆçš„ URL
                            if url and url.startswith(('http://', 'https://')):
                                raw_results.append({
                                    'url': url,
                                    'title': title or 'æ— æ ‡é¢˜',
                                    'description': description or '',
                                })
                                count += 1
                    
                    # å¦‚æœæ‰¾åˆ°äº†ç»“æœï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                    if len(raw_results) > 0:
                        break
                    else:
                        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯æœç´¢å¤±è´¥ï¼Œå°è¯•é‡è¯•
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"âš ï¸  æœªæ‰¾åˆ°ç»“æœï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                            import time
                            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                        else:
                            break
                        
                except StopIteration:
                    # è¿­ä»£å™¨ç»“æŸï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    break
                except Exception as retry_error:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"âš ï¸  æœç´¢å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    else:
                        raise retry_error
            
            if len(raw_results) == 0:
                raise RuntimeError(
                    "æœªèƒ½è·å–åˆ°æœç´¢ç»“æœã€‚\n"
                    "å¯èƒ½çš„åŸå› ï¼š\n"
                    "1. ç½‘ç»œè¿æ¥é—®é¢˜\n"
                    "2. DuckDuckGo æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n"
                    "3. æœç´¢å…³é”®è¯å¯èƒ½è¢«é™åˆ¶\n"
                    "\n"
                    "å»ºè®®ï¼š\n"
                    "1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
                    "2. ç¨åé‡è¯•\n"
                    "3. å°è¯•ä½¿ç”¨ä¸åŒçš„æœç´¢å…³é”®è¯"
                )
            
            filtered_results = self._è¿‡æ»¤æœç´¢ç»“æœ(æŸ¥è¯¢, raw_results, ç»“æœæ•°é‡)
            if len(filtered_results) < ç»“æœæ•°é‡:
                print(f"âš ï¸  ä»…æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³ç»“æœï¼ˆç›®æ ‡ {ç»“æœæ•°é‡} ä¸ªï¼‰")
            print(f"âœ“ æ‰¾åˆ° {len(filtered_results)} ä¸ªç»“æœ")
            return filtered_results
            
        except RuntimeError:
            # é‡æ–°æŠ›å‡º RuntimeErrorï¼ˆå·²ç»åŒ…å«å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼‰
            raise
        except Exception as e:
            error_msg = str(e)
            raise RuntimeError(
                f"å…è´¹æœç´¢å¤±è´¥: {error_msg}\n"
                "\n"
                "å»ºè®®ï¼š\n"
                "1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
                "2. æ›´æ–°æœç´¢åº“: pip install --upgrade ddgs\n"
                "3. ç¨åé‡è¯•"
            )
    
    def è·å–ç½‘ç«™åœ°å›¾(
        self,
        url: str,
        æœç´¢å…³é”®è¯: Optional[str] = None,
        æœ€å¤§é“¾æ¥æ•°: Optional[int] = None,
        **kwargs
    ) -> List[Dict[str, str]]:
        """
        è·å–ç½‘ç«™çš„æ‰€æœ‰é“¾æ¥ï¼ˆç½‘ç«™åœ°å›¾ï¼‰
        
        å‚æ•°:
            url: ç½‘ç«™ URL
            æœç´¢å…³é”®è¯: å¯é€‰ï¼Œç”¨äºè¿‡æ»¤é“¾æ¥
            æœ€å¤§é“¾æ¥æ•°: æœ€å¤šè¿”å›çš„é“¾æ¥æ•°é‡
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            é“¾æ¥åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¥åŒ…å« URLã€æ ‡é¢˜ã€æè¿°
        
        ç¤ºä¾‹:
            links = client.è·å–ç½‘ç«™åœ°å›¾("https://example.com")
            for link in links:
                print(f"{link['title']}: {link['url']}")
        """
        if not self._client:
            raise ValueError("æ­¤åŠŸèƒ½éœ€è¦ API å¯†é’¥ã€‚è¯·è®¾ç½® FIRECRAWL_API_KEYã€‚")
        
        try:
            print(f"æ­£åœ¨è·å–ç½‘ç«™åœ°å›¾: {url}")
            
            map_data = self._client.map(
                url=url,
                search=æœç´¢å…³é”®è¯,
                limit=æœ€å¤§é“¾æ¥æ•°,
                **kwargs
            )
            
            links = getattr(map_data, 'links', []) or []
            print(f"âœ“ æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
            
            # è½¬æ¢ä¸ºå‹å¥½çš„æ ¼å¼
            results = []
            for link in links:
                results.append({
                    'url': getattr(link, 'url', ''),
                    'title': getattr(link, 'title', ''),
                    'description': getattr(link, 'description', ''),
                })
            
            return results
        except Exception as e:
            error_msg = str(e)
            raise RuntimeError(f"è·å–ç½‘ç«™åœ°å›¾å¤±è´¥: {error_msg}")
    
    def æ‰¹é‡æŠ“å–(
        self,
        urls: List[str],
        æ ¼å¼: Optional[List[str]] = None,
        è½®è¯¢é—´éš”: int = 2,
        è¶…æ—¶: Optional[int] = None,
        **kwargs
    ) -> List['ç½‘é¡µç»“æœ']:
        """
        æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ
        
        å‚æ•°:
            urls: URL åˆ—è¡¨
            æ ¼å¼: è¾“å‡ºæ ¼å¼åˆ—è¡¨
            è½®è¯¢é—´éš”: æ£€æŸ¥çŠ¶æ€çš„é—´éš”ï¼ˆç§’ï¼‰
            è¶…æ—¶: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            ç½‘é¡µç»“æœå¯¹è±¡åˆ—è¡¨
        
        ç¤ºä¾‹:
            urls = [
                "https://example.com/page1",
                "https://example.com/page2",
                "https://example.com/page3"
            ]
            results = client.æ‰¹é‡æŠ“å–(urls)
            for result in results:
                print(result.æ ‡é¢˜)
        """
        if æ ¼å¼ is None:
            æ ¼å¼ = ["markdown"]
        
        if not self._client:
            raise ValueError("æ­¤åŠŸèƒ½éœ€è¦ API å¯†é’¥ã€‚è¯·è®¾ç½® FIRECRAWL_API_KEYã€‚")
        
        try:
            print(f"å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} ä¸ªç½‘é¡µ...")
            
            batch_job = self._client.batch_scrape(
                urls=urls,
                formats=æ ¼å¼,
                poll_interval=è½®è¯¢é—´éš”,
                wait_timeout=è¶…æ—¶,
                **kwargs
            )
            
            print(f"âœ“ æ‰¹é‡æŠ“å–å®Œæˆ: {batch_job.completed}/{batch_job.total}")
            
            # è½¬æ¢ä¸ºå‹å¥½çš„ç»“æœåˆ—è¡¨
            results = []
            if hasattr(batch_job, 'data') and batch_job.data:
                for doc in batch_job.data:
                    results.append(ç½‘é¡µç»“æœ(doc))
            
            return results
        except Exception as e:
            error_msg = str(e)
            raise RuntimeError(f"æ‰¹é‡æŠ“å–å¤±è´¥: {error_msg}")
    
    # è‹±æ–‡æ–¹æ³•åˆ«åï¼ˆå…¼å®¹æ€§ï¼‰
    def scrape(self, url: str, **kwargs) -> 'ç½‘é¡µç»“æœ':
        """æŠ“å–å•ä¸ªç½‘é¡µï¼ˆè‹±æ–‡æ–¹æ³•ï¼‰"""
        return self.æŠ“å–ç½‘é¡µ(url, **kwargs)
    
    def crawl(self, url: str, **kwargs) -> List['ç½‘é¡µç»“æœ']:
        """çˆ¬å–ç½‘ç«™ï¼ˆè‹±æ–‡æ–¹æ³•ï¼‰"""
        return self.çˆ¬å–ç½‘ç«™(url, **kwargs)
    
    def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """æœç´¢ç½‘é¡µï¼ˆè‹±æ–‡æ–¹æ³•ï¼‰"""
        return self.æœç´¢ç½‘é¡µ(query, **kwargs)
    
    def map(self, url: str, **kwargs) -> List[Dict[str, str]]:
        """è·å–ç½‘ç«™åœ°å›¾ï¼ˆè‹±æ–‡æ–¹æ³•ï¼‰"""
        return self.è·å–ç½‘ç«™åœ°å›¾(url, **kwargs)
    
    def batch_scrape(self, urls: List[str], **kwargs) -> List['ç½‘é¡µç»“æœ']:
        """æ‰¹é‡æŠ“å–ï¼ˆè‹±æ–‡æ–¹æ³•ï¼‰"""
        return self.æ‰¹é‡æŠ“å–(urls, **kwargs)
    
    def æå–æ–‡ç« ä¿¡æ¯(
        self,
        url: str,
        ä½¿ç”¨æœ¬åœ°æå–: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æå–æ–‡ç« çš„æ ¸å¿ƒä¿¡æ¯ï¼šæ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€ä¸»æ–‡
        
        å‚æ•°:
            url: è¦æå–çš„æ–‡ç«  URL
            ä½¿ç”¨æœ¬åœ°æå–: å¦‚æœä¸º Trueï¼Œä½¿ç”¨æœ¬åœ°æå–ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
        è¿”å›:
            åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€ä¸»æ–‡çš„å­—å…¸
        
        ç¤ºä¾‹:
            # ä½¿ç”¨æœ¬åœ°æå–ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
            article = client.æå–æ–‡ç« ä¿¡æ¯("https://example.com/article", ä½¿ç”¨æœ¬åœ°æå–=True)
            
            # ä½¿ç”¨ Firecrawl API æå–ï¼ˆéœ€è¦ API å¯†é’¥ï¼Œæ›´å‡†ç¡®ï¼‰
            article = client.æå–æ–‡ç« ä¿¡æ¯("https://example.com/article")
        """
        # å¦‚æœä½¿ç”¨æœ¬åœ°æå–ï¼Œä½¿ç”¨æœ¬åœ°æ–¹æ³•
        if ä½¿ç”¨æœ¬åœ°æå–:
            return self._æœ¬åœ°æå–æ–‡ç« ä¿¡æ¯(url)
        
        # å¦åˆ™ä½¿ç”¨ Firecrawl APIï¼ˆéœ€è¦ API å¯†é’¥ï¼‰
        if not self._client:
            # å¦‚æœæ²¡æœ‰ API å¯†é’¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æå–
            print("ğŸ’¡ æœªè®¾ç½® API å¯†é’¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æå–ï¼ˆå…è´¹ï¼‰")
            return self._æœ¬åœ°æå–æ–‡ç« ä¿¡æ¯(url)
        
        # å®šä¹‰æå–çš„ JSON Schema
        schema = {
            "type": "object",
            "properties": {
                "title": {
                    "type": "string",
                    "description": "æ–‡ç« æ ‡é¢˜"
                },
                "author": {
                    "type": "string",
                    "description": "æ–‡ç« ä½œè€…ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"
                },
                "publish_time": {
                    "type": "string",
                    "description": "æ–‡ç« å‘è¡¨æ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SSï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"
                },
                "content": {
                    "type": "string",
                    "description": "æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå»é™¤å¯¼èˆªæ ã€é¡µè„šã€å¹¿å‘Šç­‰æ— å…³ä¿¡æ¯ï¼Œåªä¿ç•™æ­£æ–‡"
                }
            },
            "required": ["title", "content"]
        }
        
        # å®šä¹‰æå–æç¤º
        prompt = (
            "ä»ç½‘é¡µä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š\n"
            "1. æ ‡é¢˜ï¼šæ–‡ç« çš„æ ‡é¢˜\n"
            "2. ä½œè€…ï¼šæ–‡ç« çš„ä½œè€…ï¼ˆå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰\n"
            "3. å‘è¡¨æ—¶é—´ï¼šæ–‡ç« çš„å‘è¡¨æˆ–æ›´æ–°æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SSï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰\n"
            "4. ä¸»æ–‡ï¼šæ–‡ç« çš„æ­£æ–‡å†…å®¹ï¼Œå»é™¤æ‰€æœ‰æ— å…³ä¿¡æ¯ï¼ˆå¯¼èˆªæ ã€é¡µè„šã€å¹¿å‘Šã€ä¾§è¾¹æ ã€è¯„è®ºç­‰ï¼‰ï¼Œåªä¿ç•™æ­£æ–‡å†…å®¹\n\n"
            "æ³¨æ„ï¼šåªæå–ä¸Šè¿°å››ä¸ªå­—æ®µï¼Œå…¶ä»–ä¿¡æ¯ä¸è¦æå–ã€‚"
        )
        
        try:
            print(f"æ­£åœ¨ä½¿ç”¨ Firecrawl API æå–æ–‡ç« ä¿¡æ¯: {url}")
            print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨å€™...")
            
            # åˆ›å»º ScrapeOptions å¯¹è±¡
            scrape_options = ScrapeOptions(
                formats=["markdown"],
                only_main_content=True
            )
            
            # ä½¿ç”¨ extract API æå–ç»“æ„åŒ–æ•°æ®
            extract_response = self._client.extract(
                urls=[url],
                prompt=prompt,
                schema=schema,
                scrape_options=scrape_options,
                **kwargs
            )
            
            # æ£€æŸ¥æå–ç»“æœ
            if hasattr(extract_response, 'data') and extract_response.data:
                data = extract_response.data
                # å¦‚æœ data æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if isinstance(data, list) and len(data) > 0:
                    data = data[0]
                
                # ç¡®ä¿è¿”å›çš„å­—å…¸åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
                result = {
                    "title": data.get("title", ""),
                    "author": data.get("author", ""),
                    "publish_time": data.get("publish_time", ""),
                    "content": data.get("content", ""),
                    "url": url
                }
                
                print("âœ“ æå–æˆåŠŸ")
                return result
            else:
                raise RuntimeError("æå–ç»“æœä¸ºç©º")
        
        except Exception as e:
            error_msg = str(e)
            # å¦‚æœ API æå–å¤±è´¥ï¼Œå°è¯•æœ¬åœ°æå–
            print(f"âš ï¸  API æå–å¤±è´¥: {error_msg}")
            print("ğŸ’¡ å°è¯•ä½¿ç”¨æœ¬åœ°æå–...")
            return self._æœ¬åœ°æå–æ–‡ç« ä¿¡æ¯(url)
    
    def _æœ¬åœ°æå–æ–‡ç« ä¿¡æ¯(self, url: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨æœ¬åœ°æ–¹æ³•æå–æ–‡ç« ä¿¡æ¯ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
        
        å‚æ•°:
            url: è¦æå–çš„æ–‡ç«  URL
        
        è¿”å›:
            åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€ä¸»æ–‡çš„å­—å…¸
        """
        if not LOCAL_EXTRACT_AVAILABLE:
            error_msg = (
                "æœ¬åœ°æå–åº“æœªå®‰è£…ã€‚\n"
                "è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                "  pip install readability-lxml beautifulsoup4 html2text python-dateutil lxml\n"
                "\n"
                "æˆ–è€…åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ï¼š\n"
                "  source venv/bin/activate\n"
                "  pip install readability-lxml beautifulsoup4 html2text python-dateutil lxml\n"
                "\n"
                "æˆ–è€…å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š\n"
                "  pip install -r requirements.txt"
            )
            raise RuntimeError(error_msg)
        
        try:
            print(f"æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ–¹æ³•æå–æ–‡ç« ä¿¡æ¯: {url}")
            print("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯æœ¬åœ°æå–ï¼Œä¸éœ€è¦ API å¯†é’¥")
            
            # ä¸‹è½½ç½‘é¡µï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            max_retries = 3
            retry_count = 0
            html = None
            
            while retry_count < max_retries:
                try:
                    response = requests.get(
                        url,
                        headers=headers,
                        timeout=30,
                        allow_redirects=True
                    )
                    response.raise_for_status()
                    
                    encoding = response.encoding
                    if not encoding or encoding.lower() == 'iso-8859-1':
                        encoding = response.apparent_encoding or 'utf-8'
                    response.encoding = encoding or 'utf-8'
                    html = response.text or response.content.decode(response.encoding, errors='replace')
                    break
                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 403:
                        raise RuntimeError(f"è®¿é—®è¢«æ‹’ç» (403): è¯¥ç½‘ç«™å¯èƒ½é˜»æ­¢äº†è‡ªåŠ¨è®¿é—®ã€‚\nURL: {url}")
                    elif e.response.status_code == 404:
                        raise RuntimeError(f"é¡µé¢ä¸å­˜åœ¨ (404): {url}")
                    elif e.response.status_code >= 500:
                        retry_count += 1
                        if retry_count < max_retries:
                            print(f"  âš ï¸  æœåŠ¡å™¨é”™è¯¯ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                            import time
                            time.sleep(2)
                            continue
                        else:
                            raise RuntimeError(f"æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code}): {url}")
                    else:
                        raise RuntimeError(f"HTTP é”™è¯¯ ({e.response.status_code}): {url}")
                except requests.exceptions.ConnectionError as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"  âš ï¸  è¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        raise RuntimeError(f"è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚\nURL: {url}\né”™è¯¯: {str(e)}")
                except requests.exceptions.Timeout:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"  âš ï¸  è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        raise RuntimeError(f"è¯·æ±‚è¶…æ—¶: {url}")
            
            if html is None:
                raise RuntimeError("æ— æ³•è·å–ç½‘é¡µå†…å®¹")
            
            # ä½¿ç”¨ readability æå–ä¸»è¦å†…å®¹
            # ä¿®å¤ Pydantic å…¼å®¹æ€§é—®é¢˜
            try:
                # æ–¹å¼1ï¼šç›´æ¥ä¼ é€’ HTML å­—ç¬¦ä¸²ï¼ˆæœ€å¸¸è§ï¼‰
                doc = ReadabilityDocument(html)
            except (TypeError, ValueError) as e:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ lxml è§£æ
                try:
                    from lxml import html as lxml_html
                    doc_html = lxml_html.fromstring(html.encode('utf-8'))
                    doc = ReadabilityDocument(doc_html)
                except Exception as e2:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä¼ é€’å­—èŠ‚
                    try:
                        doc = ReadabilityDocument(html.encode('utf-8'))
                    except Exception as e3:
                        raise RuntimeError(
                            f"æ— æ³•è§£æ HTML å†…å®¹ã€‚\n"
                            f"é”™è¯¯1: {str(e)}\n"
                            f"é”™è¯¯2: {str(e2)}\n"
                            f"é”™è¯¯3: {str(e3)}\n"
                            f"å»ºè®®ï¼šæ£€æŸ¥ HTML å†…å®¹æ˜¯å¦æœ‰æ•ˆ"
                        )
            
            title = self._æ¸…ç†æ–‡æœ¬(doc.title() if hasattr(doc, 'title') else "")
            content_html = doc.summary() if hasattr(doc, 'summary') else ""
            
            # åˆ›å»º BeautifulSoup å¯¹è±¡ï¼ˆç”¨äºåç»­æå–ï¼‰
            original_soup = BeautifulSoup(html, 'lxml')
            
            # å¦‚æœ readability æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            if not content_html or len(content_html.strip()) < 50:
                fallback_soup = BeautifulSoup(html, 'lxml')
                for script in fallback_soup(["script", "style", "nav", "header", "footer", "aside"]):
                    script.decompose()
                content_html = str(fallback_soup.find('body') or fallback_soup)
            
            # è§£æ HTML
            soup = BeautifulSoup(content_html, 'lxml')
            
            # è½¬æ¢ä¸º Markdown
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            h.body_width = 0
            content = h.handle(str(soup))
            content = self._æ¸…ç†æ–‡æœ¬(content)
            
            # æå–ä½œè€…ï¼ˆå°è¯•å¤šç§å¸¸è§çš„ meta æ ‡ç­¾å’Œå±æ€§ï¼‰
            author = ""
            author_selectors = [
                ('meta', {'name': 'author'}),
                ('meta', {'property': 'article:author'}),
                ('meta', {'property': 'og:article:author'}),
                ('meta', {'name': 'twitter:creator'}),
                ('span', {'class': 'author'}),
                ('span', {'class': 'by-author'}),
                ('div', {'class': 'author'}),
                ('a', {'rel': 'author'}),
            ]
            for tag_name, attrs in author_selectors:
                tags = original_soup.find_all(tag_name, attrs)
                if tags:
                    for tag in tags:
                        if tag_name == 'meta':
                            author = tag.get('content', '')
                        else:
                            author = tag.get_text(strip=True)
                        if author:
                            break
                if author:
                    break
            
            # æå–å‘è¡¨æ—¶é—´ï¼ˆå°è¯•å¤šç§å¸¸è§çš„ meta æ ‡ç­¾å’Œå±æ€§ï¼‰
            publish_time = ""
            time_selectors = [
                ('meta', {'property': 'article:published_time'}),
                ('meta', {'property': 'article:modified_time'}),
                ('meta', {'name': 'publishdate'}),
                ('meta', {'name': 'pubdate'}),
                ('time', {'datetime': True}),
                ('time', {'pubdate': True}),
            ]
            
            for tag_name, attrs in time_selectors:
                tags = original_soup.find_all(tag_name, attrs)
                if tags:
                    for tag in tags:
                        if tag_name == 'meta':
                            time_str = tag.get('content', '')
                        else:
                            time_str = tag.get('datetime', '')
                        
                        if time_str:
                            try:
                                # å°è¯•è§£ææ—¥æœŸ
                                dt = date_parser.parse(time_str)
                                publish_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                            except:
                                publish_time = time_str
                            break
                if publish_time:
                    break
            
            # æ¸…ç†å†…å®¹
            content = content.strip()
            if not content:
                # å¦‚æœ readability æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œå°è¯•æå– body
                body = original_soup.find('body')
                if body:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in body(["script", "style", "nav", "header", "footer", "aside"]):
                        script.decompose()
                    content = h.handle(str(body))
                    content = self._æ¸…ç†æ–‡æœ¬(content)
            
            result = {
                "title": title,
                "author": author.strip() if author else "",
                "publish_time": publish_time.strip() if publish_time else "",
                "content": content,
                "url": url
            }
            
            print("âœ“ æœ¬åœ°æå–æˆåŠŸ")
            return result
            
        except Exception as e:
            error_msg = str(e)
            raise RuntimeError(f"æœ¬åœ°æå–å¤±è´¥: {error_msg}")


class ç½‘é¡µç»“æœ:
    """
    ç½‘é¡µæŠ“å–ç»“æœ
    
    è¿™ä¸ªç±»å°è£…äº†æŠ“å–åˆ°çš„ç½‘é¡µæ•°æ®ï¼Œæä¾›äº†å‹å¥½çš„å±æ€§è®¿é—®æ–¹å¼ã€‚
    """
    
    def __init__(self, document: Document):
        """
        åˆå§‹åŒ–ç½‘é¡µç»“æœ
        
        å‚æ•°:
            document: Firecrawl Document å¯¹è±¡
        """
        self._document = document
        self._metadata = document.metadata_dict if hasattr(document, 'metadata_dict') else {}
    
    @property
    def å†…å®¹(self) -> str:
        """è·å– Markdown æ ¼å¼çš„å†…å®¹"""
        return getattr(self._document, 'markdown', '') or ''
    
    @property
    def HTML(self) -> str:
        """è·å– HTML æ ¼å¼çš„å†…å®¹"""
        return getattr(self._document, 'html', '') or ''
    
    @property
    def æ ‡é¢˜(self) -> str:
        """è·å–ç½‘é¡µæ ‡é¢˜"""
        return self._metadata.get('title', '') or self._metadata.get('og_title', '') or ''
    
    @property
    def æè¿°(self) -> str:
        """è·å–ç½‘é¡µæè¿°"""
        return self._metadata.get('description', '') or self._metadata.get('og_description', '') or ''
    
    @property
    def URL(self) -> str:
        """è·å–ç½‘é¡µ URL"""
        return self._metadata.get('source_url', '') or self._metadata.get('url', '') or ''
    
    @property
    def å…ƒæ•°æ®(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„å…ƒæ•°æ®å­—å…¸"""
        return self._metadata.copy()
    
    @property
    def é“¾æ¥(self) -> List[str]:
        """è·å–ç½‘é¡µä¸­çš„æ‰€æœ‰é“¾æ¥"""
        links = getattr(self._document, 'links', []) or []
        return [link.url if hasattr(link, 'url') else str(link) for link in links]
    
    def ä¿å­˜ä¸ºæ–‡ä»¶(self, æ–‡ä»¶è·¯å¾„: str, æ ¼å¼: str = 'markdown'):
        """
        å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
        
        å‚æ•°:
            æ–‡ä»¶è·¯å¾„: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            æ ¼å¼: æ–‡ä»¶æ ¼å¼ï¼Œå¯é€‰å€¼ï¼š'markdown', 'html', 'txt'
        
        ç¤ºä¾‹:
            result.ä¿å­˜ä¸ºæ–‡ä»¶("output.md", "markdown")
        """
        if æ ¼å¼.lower() == 'markdown':
            content = self.å†…å®¹
            ext = '.md'
        elif æ ¼å¼.lower() == 'html':
            content = self.HTML
            ext = '.html'
        else:
            content = self.å†…å®¹
            ext = '.txt'
        
        # ç¡®ä¿æ–‡ä»¶æ‰©å±•åæ­£ç¡®
        if not æ–‡ä»¶è·¯å¾„.endswith(ext):
            æ–‡ä»¶è·¯å¾„ += ext
        
        try:
            with open(æ–‡ä»¶è·¯å¾„, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ“ å·²ä¿å­˜åˆ°: {æ–‡ä»¶è·¯å¾„}")
        except Exception as e:
            raise IOError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def __str__(self) -> str:
        """è¿”å›ç»“æœçš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"ç½‘é¡µç»“æœ(æ ‡é¢˜='{self.æ ‡é¢˜}', URL='{self.URL}')"
    
    def __repr__(self) -> str:
        """è¿”å›ç»“æœçš„è¯¦ç»†è¡¨ç¤º"""
        return f"ç½‘é¡µç»“æœ(æ ‡é¢˜='{self.æ ‡é¢˜}', URL='{self.URL}', å†…å®¹é•¿åº¦={len(self.å†…å®¹)})"


# ä¾¿æ·å‡½æ•°
def å¿«é€ŸæŠ“å–(url: str, api_key: Optional[str] = None) -> ç½‘é¡µç»“æœ:
    """
    å¿«é€ŸæŠ“å–å•ä¸ªç½‘é¡µçš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        url: è¦æŠ“å–çš„ URL
        api_key: API å¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    
    è¿”å›:
        ç½‘é¡µç»“æœå¯¹è±¡
    
    ç¤ºä¾‹:
        result = å¿«é€ŸæŠ“å–("https://example.com")
        print(result.å†…å®¹)
    """
    client = FirecrawlClient(api_key=api_key)
    return client.æŠ“å–ç½‘é¡µ(url)


# ä¸»ç¨‹åºç¤ºä¾‹
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Firecrawl å‹å¥½å®¢æˆ·ç«¯ç¤ºä¾‹')
    parser.add_argument('--url', type=str, help='è¦æŠ“å–çš„ URL')
    parser.add_argument('--api-key', type=str, help='API å¯†é’¥')
    parser.add_argument('--action', type=str, choices=['scrape', 'crawl', 'search', 'map'], 
                       default='scrape', help='æ“ä½œç±»å‹')
    parser.add_argument('--query', type=str, help='æœç´¢å…³é”®è¯ï¼ˆç”¨äº search æ“ä½œï¼‰')
    
    args = parser.parse_args()
    
    try:
        client = FirecrawlClient(api_key=args.api_key)
        
        if args.action == 'scrape' and args.url:
            result = client.æŠ“å–ç½‘é¡µ(args.url)
            print(f"\næ ‡é¢˜: {result.æ ‡é¢˜}")
            print(f"URL: {result.URL}")
            print(f"\nå†…å®¹é¢„è§ˆ:\n{result.å†…å®¹[:500]}...")
        
        elif args.action == 'crawl' and args.url:
            results = client.çˆ¬å–ç½‘ç«™(args.url, æœ€å¤§é¡µé¢æ•°=5)
            print(f"\nå…±æŠ“å– {len(results)} ä¸ªé¡µé¢:")
            for i, result in enumerate(results, 1):
                print(f"{i}. {result.æ ‡é¢˜} - {result.URL}")
        
        elif args.action == 'search' and args.query:
            results = client.æœç´¢ç½‘é¡µ(args.query, ç»“æœæ•°é‡=5)
            print(f"\næœç´¢ç»“æœ:")
            for i, result in enumerate(results, 1):
                print(f"{i}. {result['title']}")
                print(f"   {result['url']}")
                print(f"   {result['description']}\n")
        
        elif args.action == 'map' and args.url:
            links = client.è·å–ç½‘ç«™åœ°å›¾(args.url, æœ€å¤§é“¾æ¥æ•°=10)
            print(f"\næ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥:")
            for i, link in enumerate(links, 1):
                print(f"{i}. {link['title']} - {link['url']}")
        
        else:
            print("è¯·æä¾›å¿…è¦çš„å‚æ•°ã€‚ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚")
    
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

