#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Firecrawl å‹å¥½å®¢æˆ·ç«¯ - äº¤äº’å¼å‘½ä»¤è¡Œå·¥å…·
========================================

ä¸€ä¸ªç®€å•æ˜“ç”¨çš„äº¤äº’å¼å‘½ä»¤è¡Œå·¥å…·ï¼Œè®©æ‚¨è½»æ¾ä½¿ç”¨ Firecrawl çš„å„ç§åŠŸèƒ½ã€‚
"""

import os
import sys
import json
import signal
from typing import Optional
from datetime import datetime

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
def check_venv():
    """æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­"""
    in_venv = (
        hasattr(sys, 'real_prefix') or
        (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
    )
    return in_venv

# å¯¼å…¥å®¢æˆ·ç«¯
try:
    from firecrawl_client import FirecrawlClient, å¿«é€ŸæŠ“å–, ç½‘é¡µç»“æœ
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("\nè¯·ç¡®ä¿:")
    print("1. å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source venv/bin/activate")
    print("2. æˆ–ä½¿ç”¨å¯åŠ¨è„šæœ¬: bash run_examples.sh")
    sys.exit(1)


def clear_screen():
    """æ¸…å±"""
    os.system('clear' if os.name != 'nt' else 'cls')


def print_header():
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print("ğŸ”¥ Firecrawl å‹å¥½å®¢æˆ·ç«¯ - äº¤äº’å¼å·¥å…·")
    print("=" * 60)
    print()


def print_menu():
    """æ‰“å°ä¸»èœå•"""
    print("\n" + "-" * 60)
    print("è¯·é€‰æ‹©åŠŸèƒ½ï¼š")
    print("-" * 60)
    print("1. ğŸ“„ æŠ“å–å•ä¸ªç½‘é¡µ")
    print("2. ğŸ“° æå–æ–‡ç« ä¿¡æ¯ï¼ˆä»…æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€æ­£æ–‡ï¼‰â­")
    print("3. ğŸ•·ï¸  çˆ¬å–æ•´ä¸ªç½‘ç«™")
    print("4. ğŸ” æœç´¢ç½‘é¡µ")
    print("5. ğŸ—ºï¸  è·å–ç½‘ç«™åœ°å›¾ï¼ˆæ‰€æœ‰é“¾æ¥ï¼‰")
    print("6. ğŸ“¦ æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ")
    print("7. âš™ï¸  è®¾ç½® API å¯†é’¥")
    print("8. â„¹ï¸  æŸ¥çœ‹ä½¿ç”¨è¯´æ˜")
    print("0. ğŸšª é€€å‡º")
    print("-" * 60)


def get_user_input(prompt: str, default: Optional[str] = None) -> str:
    """è·å–ç”¨æˆ·è¾“å…¥"""
    if default:
        user_input = input(f"{prompt} (é»˜è®¤: {default}): ").strip()
        return user_input if user_input else default
    else:
        while True:
            user_input = input(f"{prompt}: ").strip()
            if user_input:
                return user_input
            print("âš ï¸  è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")


def check_api_key() -> bool:
    """æ£€æŸ¥ API å¯†é’¥æ˜¯å¦è®¾ç½®"""
    api_key = os.getenv("FIRECRAWL_API_KEY")
    if not api_key:
        print("\nâš ï¸  æœªè®¾ç½® API å¯†é’¥ï¼")
        print("\nè¯·é€‰æ‹©ï¼š")
        print("1. ç°åœ¨è®¾ç½® API å¯†é’¥")
        print("2. ç¨åè®¾ç½®ï¼ˆæŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨ï¼‰")
        choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
        
        if choice == "1":
            set_api_key()
            return True
        else:
            return False
    return True


def set_api_key():
    """è®¾ç½® API å¯†é’¥"""
    print("\n" + "=" * 60)
    print("è®¾ç½® API å¯†é’¥")
    print("=" * 60)
    print("\nè·å– API å¯†é’¥: https://firecrawl.dev")
    print("API å¯†é’¥é€šå¸¸ä»¥ 'fc-' å¼€å¤´")
    print()
    
    api_key = get_user_input("è¯·è¾“å…¥æ‚¨çš„ API å¯†é’¥")
    
    # éªŒè¯æ ¼å¼
    if not api_key.startswith("fc-"):
        print("âš ï¸  è­¦å‘Š: API å¯†é’¥é€šå¸¸ä»¥ 'fc-' å¼€å¤´")
        confirm = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå½“å‰ä¼šè¯ï¼‰
    os.environ["FIRECRAWL_API_KEY"] = api_key
    print(f"\nâœ“ API å¯†é’¥å·²è®¾ç½®ï¼ˆå½“å‰ä¼šè¯æœ‰æ•ˆï¼‰")
    print("\næç¤º: è¦æ°¸ä¹…ä¿å­˜ï¼Œè¯·è¿è¡Œ:")
    print(f"  export FIRECRAWL_API_KEY='{api_key}'")
    print("  æˆ–åˆ›å»º .env æ–‡ä»¶")


def extract_article():
    """æå–æ–‡ç« ä¿¡æ¯ï¼ˆä»…æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€æ­£æ–‡ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“° æå–æ–‡ç« ä¿¡æ¯")
    print("=" * 60)
    print("å°†æå–ï¼šæ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ­£æ–‡å†…å®¹")
    print("ï¼ˆè‡ªåŠ¨å»é™¤å¯¼èˆªæ ã€é¡µè„šã€å¹¿å‘Šç­‰æ— å…³ä¿¡æ¯ï¼‰")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    url = get_user_input("è¯·è¾“å…¥æ–‡ç«  URL")
    
    print(f"\næ­£åœ¨æå–æ–‡ç« ä¿¡æ¯: {url}")
    print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        article = client.æå–æ–‡ç« ä¿¡æ¯(url)
        
        print("\n" + "=" * 60)
        print("âœ“ æå–æˆåŠŸï¼")
        print("=" * 60)
        
        print(f"\nğŸ“Œ æ ‡é¢˜: {article.get('title', 'æœªæ‰¾åˆ°')}")
        print(f"âœï¸  ä½œè€…: {article.get('author', 'æœªæ‰¾åˆ°') or 'æœªæ‰¾åˆ°'}")
        print(f"ğŸ“… å‘è¡¨æ—¶é—´: {article.get('publish_time', 'æœªæ‰¾åˆ°') or 'æœªæ‰¾åˆ°'}")
        print(f"ğŸ”— URL: {article.get('url', url)}")
        
        content = article.get('content', '')
        if content:
            print(f"\nğŸ“ æ­£æ–‡å†…å®¹ ({len(content)} å­—ç¬¦):")
            print("-" * 60)
            # æ˜¾ç¤ºå‰ 500 å­—ç¬¦é¢„è§ˆ
            preview = content[:500]
            print(preview)
            if len(content) > 500:
                print(f"... (è¿˜æœ‰ {len(content) - 500} å­—ç¬¦)")
            print("-" * 60)
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹")
        
        # è¯¢é—®æ˜¯å¦ä¿å­˜
        save = input("\næ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶ï¼Ÿ(y/N): ").strip().lower()
        if save == 'y':
            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = "".join(c for c in article.get('title', 'article') if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
            safe_title = safe_title.replace(' ', '_')
            filename = f"article_{safe_title}.md"
            
            # ä¿å­˜å†…å®¹
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(f"# {article.get('title', 'æ— æ ‡é¢˜')}\n\n")
                    if article.get('author'):
                        f.write(f"**ä½œè€…**: {article.get('author')}\n\n")
                    if article.get('publish_time'):
                        f.write(f"**å‘è¡¨æ—¶é—´**: {article.get('publish_time')}\n\n")
                    f.write(f"**URL**: {article.get('url', url)}\n\n")
                    f.write("---\n\n")
                    f.write(article.get('content', ''))
                print(f"\nâœ“ å·²ä¿å­˜åˆ°: {filename}")
                print(f"  å®Œæ•´è·¯å¾„: {os.path.abspath(filename)}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜å¤±è´¥: {e}")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def scrape_single_page():
    """æŠ“å–å•ä¸ªç½‘é¡µ"""
    print("\n" + "=" * 60)
    print("ğŸ“„ æŠ“å–å•ä¸ªç½‘é¡µ")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    url = get_user_input("è¯·è¾“å…¥è¦æŠ“å–çš„ç½‘é¡µ URL")
    
    print("\né€‰æ‹©è¾“å‡ºæ ¼å¼:")
    print("1. Markdown (æ¨è)")
    print("2. HTML")
    print("3. Markdown + HTML")
    format_choice = input("è¯·é€‰æ‹© (1-3, é»˜è®¤: 1): ").strip() or "1"
    
    formats_map = {
        "1": ["markdown"],
        "2": ["html"],
        "3": ["markdown", "html"]
    }
    formats = formats_map.get(format_choice, ["markdown"])
    
    print(f"\næ­£åœ¨æŠ“å–: {url}")
    print("è¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        result = client.æŠ“å–ç½‘é¡µ(url, æ ¼å¼=formats)
        
        print("\n" + "=" * 60)
        print("âœ“ æŠ“å–æˆåŠŸï¼")
        print("=" * 60)
        print(f"\næ ‡é¢˜: {result.æ ‡é¢˜}")
        print(f"URL: {result.URL}")
        if result.æè¿°:
            print(f"æè¿°: {result.æè¿°}")
        print(f"\nå†…å®¹é•¿åº¦: {len(result.å†…å®¹)} å­—ç¬¦")
        
        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        preview = result.å†…å®¹[:500] if result.å†…å®¹ else ""
        if preview:
            print(f"\nå†…å®¹é¢„è§ˆ:\n{'-' * 60}")
            print(preview)
            if len(result.å†…å®¹) > 500:
                print(f"... (è¿˜æœ‰ {len(result.å†…å®¹) - 500} å­—ç¬¦)")
        
        # è¯¢é—®æ˜¯å¦ä¿å­˜
        save = input("\næ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶ï¼Ÿ(y/N): ").strip().lower()
        if save == 'y':
            filename = get_user_input("è¯·è¾“å…¥æ–‡ä»¶å", "output.md")
            result.ä¿å­˜ä¸ºæ–‡ä»¶(filename, "markdown")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def crawl_website():
    """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  çˆ¬å–æ•´ä¸ªç½‘ç«™")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    url = get_user_input("è¯·è¾“å…¥èµ·å§‹ URL")
    max_pages = int(get_user_input("æœ€å¤šçˆ¬å–é¡µé¢æ•°", "10"))
    
    print(f"\nå¼€å§‹çˆ¬å–: {url}")
    print(f"æœ€å¤šçˆ¬å–: {max_pages} ä¸ªé¡µé¢")
    print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        results = client.çˆ¬å–ç½‘ç«™(url, æœ€å¤§é¡µé¢æ•°=max_pages)
        
        print("\n" + "=" * 60)
        print(f"âœ“ çˆ¬å–å®Œæˆï¼å…± {len(results)} ä¸ªé¡µé¢")
        print("=" * 60)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.æ ‡é¢˜}")
            print(f"   URL: {result.URL}")
            print(f"   å†…å®¹é•¿åº¦: {len(result.å†…å®¹)} å­—ç¬¦")
        
        # è¯¢é—®æ˜¯å¦ä¿å­˜æ‰€æœ‰ç»“æœ
        save = input(f"\næ˜¯å¦ä¿å­˜æ‰€æœ‰ {len(results)} ä¸ªé¡µé¢ï¼Ÿ(y/N): ").strip().lower()
        if save == 'y':
            for i, result in enumerate(results, 1):
                safe_title = "".join(c for c in result.æ ‡é¢˜ if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
                filename = f"page_{i:03d}_{safe_title}.md"
                result.ä¿å­˜ä¸ºæ–‡ä»¶(filename, "markdown")
                print(f"  å·²ä¿å­˜: {filename}")
            print(f"\nâœ“ æ‰€æœ‰é¡µé¢å·²ä¿å­˜")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def search_web():
    """æœç´¢ç½‘é¡µ"""
    print("\n" + "=" * 60)
    print("ğŸ” æœç´¢ç½‘é¡µ")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    query = get_user_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯")
    limit = int(get_user_input("è¿”å›ç»“æœæ•°é‡", "5"))
    
    # è¯¢é—®å¤„ç†æ–¹å¼
    print("\nè¯·é€‰æ‹©å¤„ç†æ–¹å¼ï¼š")
    print("1. ä»…æ˜¾ç¤ºé“¾æ¥å’Œæè¿°ï¼ˆå¿«é€Ÿï¼‰")
    print("2. æŠ“å–å®Œæ•´å†…å®¹ï¼ˆè¾ƒæ…¢ï¼Œä½†å¯ä»¥ä¿å­˜ï¼‰")
    print("3. æå–æ–‡ç« ä¿¡æ¯ï¼ˆä»…æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€æ­£æ–‡ï¼‰â­")
    mode_choice = input("è¯·é€‰æ‹© (1/2/3, é»˜è®¤: 1): ").strip() or "1"
    
    print(f"\næ­£åœ¨æœç´¢: {query}")
    print("è¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        
        # å…ˆè·å–æœç´¢ç»“æœï¼ˆä¸æŠ“å–å†…å®¹ï¼‰
        results = client.æœç´¢ç½‘é¡µ(query, ç»“æœæ•°é‡=limit, æŠ“å–å†…å®¹=False)
        
        print("\n" + "=" * 60)
        print(f"âœ“ æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        print("=" * 60)
        
        # æ˜¾ç¤ºæœç´¢ç»“æœåˆ—è¡¨
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.get('title', 'æ— æ ‡é¢˜')}")
            print(f"   URL: {result.get('url', '')}")
            if result.get('description'):
                print(f"   æè¿°: {result.get('description')}")
        
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©å¤„ç†
        if mode_choice == "1":
            # ä»…æ˜¾ç¤ºï¼Œä¸åšå…¶ä»–å¤„ç†
            print("\nâœ“ æœç´¢å®Œæˆï¼ˆä»…æ˜¾ç¤ºé“¾æ¥å’Œæè¿°ï¼‰")
            return
        
        elif mode_choice == "2":
            # æŠ“å–å®Œæ•´å†…å®¹
            print(f"\næ­£åœ¨æŠ“å– {len(results)} ä¸ªç½‘é¡µçš„å®Œæ•´å†…å®¹...")
            print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨å€™...")
            
            # é‡æ–°æœç´¢å¹¶æŠ“å–å†…å®¹
            results_with_content = client.æœç´¢ç½‘é¡µ(query, ç»“æœæ•°é‡=limit, æŠ“å–å†…å®¹=True)
            
            print("\n" + "=" * 60)
            print("âœ“ æŠ“å–å®Œæˆ")
            print("=" * 60)
            
            for i, result in enumerate(results_with_content, 1):
                print(f"\n{i}. {result.get('title', 'æ— æ ‡é¢˜')}")
                print(f"   URL: {result.get('url', '')}")
                if result.get('content'):
                    content_len = len(result.get('content', ''))
                    print(f"   å†…å®¹é•¿åº¦: {content_len} å­—ç¬¦")
            
            # è¯¢é—®æ˜¯å¦ä¿å­˜
            if any(r.get('content') for r in results_with_content):
                save = input(f"\næ˜¯å¦ä¿å­˜æ‰€æœ‰ {len(results_with_content)} ä¸ªæœç´¢ç»“æœï¼Ÿ(y/N): ").strip().lower()
                if save == 'y':
                    save_dir = "search_results"
                    os.makedirs(save_dir, exist_ok=True)
                    
                    safe_query = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
                    safe_query = safe_query.replace(' ', '_')
                    
                    print(f"\næ­£åœ¨ä¿å­˜åˆ°ç›®å½•: {save_dir}/")
                    saved_count = 0
                    
                    for i, result in enumerate(results_with_content, 1):
                        if result.get('content'):
                            safe_title = "".join(c for c in result.get('title', 'æ— æ ‡é¢˜') if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
                            safe_title = safe_title.replace(' ', '_')
                            filename = os.path.join(save_dir, f"{i:03d}_{safe_query}_{safe_title}.md")
                            
                            try:
                                with open(filename, 'w', encoding='utf-8') as f:
                                    f.write(f"# {result.get('title', 'æ— æ ‡é¢˜')}\n\n")
                                    f.write(f"**URL**: {result.get('url', '')}\n\n")
                                    if result.get('description'):
                                        f.write(f"**æè¿°**: {result.get('description')}\n\n")
                                    f.write("---\n\n")
                                    f.write(result.get('content', ''))
                                print(f"  âœ“ å·²ä¿å­˜: {filename}")
                                saved_count += 1
                            except Exception as e:
                                print(f"  âœ— ä¿å­˜å¤±è´¥ {filename}: {e}")
                    
                    if saved_count > 0:
                        print(f"\nâœ“ æˆåŠŸä¿å­˜ {saved_count} ä¸ªç»“æœåˆ° {save_dir}/ ç›®å½•")
                        print(f"  å®Œæ•´è·¯å¾„: {os.path.abspath(save_dir)}")
        
        elif mode_choice == "3":
            # æå–æ–‡ç« ä¿¡æ¯ï¼ˆä»…æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€æ­£æ–‡ï¼‰
            print(f"\næ­£åœ¨æå– {len(results)} ä¸ªç»“æœçš„æ–‡ç« ä¿¡æ¯...")
            print("ï¼ˆä»…æå–ï¼šæ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ­£æ–‡ï¼‰")
            print("ğŸ’¡ æç¤ºï¼šç»“æœä¼šè‡ªåŠ¨ä¿å­˜ï¼Œå¯éšæ—¶æŒ‰ Ctrl+C å®‰å…¨é€€å‡º")
            print("=" * 60)
            
            # å‡†å¤‡ä¿å­˜ç›®å½•å’Œæ–‡ä»¶å
            save_dir = "search_results"
            os.makedirs(save_dir, exist_ok=True)
            
            safe_query = "".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
            safe_query = safe_query.replace(' ', '_')
            
            # è¿›åº¦æ–‡ä»¶è·¯å¾„
            progress_file = os.path.join(save_dir, f".progress_{safe_query}.json")
            
            # åŠ è½½å·²æœ‰è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            processed_urls = set()
            saved_count = 0
            
            if os.path.exists(progress_file):
                try:
                    with open(progress_file, 'r', encoding='utf-8') as f:
                        progress_data = json.load(f)
                        processed_urls = set(progress_data.get('processed_urls', []))
                        saved_count = progress_data.get('saved_count', 0)
                    print(f"ğŸ“‚ æ£€æµ‹åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå·²å¤„ç† {len(processed_urls)} ä¸ªï¼Œå°†ç»§ç»­å¤„ç†å‰©ä½™ {len(results) - len(processed_urls)} ä¸ª")
                except:
                    pass
            
            extracted_articles = []
            failed_urls = []
            
            # å®šä¹‰ä¿å­˜å‡½æ•°
            def save_article(item_data, index):
                """ä¿å­˜å•ä¸ªæ–‡ç« """
                article = item_data['article']
                safe_title = "".join(c for c in article.get('title', 'æ— æ ‡é¢˜') if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
                safe_title = safe_title.replace(' ', '_')
                filename = os.path.join(save_dir, f"{index:03d}_{safe_query}_{safe_title}.md")
                
                try:
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(f"# {article.get('title', 'æ— æ ‡é¢˜')}\n\n")
                        if article.get('author'):
                            f.write(f"**ä½œè€…**: {article.get('author')}\n\n")
                        if article.get('publish_time'):
                            f.write(f"**å‘è¡¨æ—¶é—´**: {article.get('publish_time')}\n\n")
                        f.write(f"**URL**: {article.get('url', '')}\n\n")
                        f.write("---\n\n")
                        f.write(article.get('content', ''))
                    return True, filename
                except Exception as e:
                    return False, str(e)
            
            # å®šä¹‰ä¿å­˜è¿›åº¦å‡½æ•°
            def save_progress():
                """ä¿å­˜è¿›åº¦"""
                try:
                    progress_data = {
                        'query': query,
                        'processed_urls': list(processed_urls),
                        'saved_count': saved_count,
                        'last_update': datetime.now().isoformat()
                    }
                    with open(progress_file, 'w', encoding='utf-8') as f:
                        json.dump(progress_data, f, ensure_ascii=False, indent=2)
                except:
                    pass
            
            # å®šä¹‰ä¸­æ–­å¤„ç†å‡½æ•°
            def handle_interrupt(signum, frame):
                """å¤„ç†ä¸­æ–­ä¿¡å·"""
                print(f"\n\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å·²æå–çš„ç»“æœ...")
                save_progress()
                if extracted_articles:
                    print(f"âœ“ å·²ä¿å­˜ {saved_count} ä¸ªç»“æœ")
                    print(f"  ä¿å­˜ç›®å½•: {os.path.abspath(save_dir)}")
                    print(f"  è¿›åº¦æ–‡ä»¶: {progress_file}")
                    print(f"\nğŸ’¡ æç¤ºï¼šä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
                print("\nğŸ‘‹ ç¨‹åºå·²å®‰å…¨é€€å‡º")
                sys.exit(0)
            
            # æ³¨å†Œä¸­æ–­å¤„ç†ï¼ˆWindows å…¼å®¹ï¼‰
            try:
                signal.signal(signal.SIGINT, handle_interrupt)
            except (AttributeError, ValueError):
                # Windows ä¸Šå¯èƒ½ä¸æ”¯æŒæŸäº›ä¿¡å·
                pass
            
            # å¼€å§‹æå–
            start_time = datetime.now()
            
            for i, result in enumerate(results, 1):
                url = result.get('url', '')
                if not url:
                    continue
                
                # è·³è¿‡å·²å¤„ç†çš„URL
                if url in processed_urls:
                    print(f"[{i}/{len(results)}] â­ï¸  å·²å¤„ç†ï¼Œè·³è¿‡: {url[:60]}...")
                    continue
                
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆç®€åŒ–è¾“å‡ºä»¥æé«˜é€Ÿåº¦ï¼‰
                elapsed = (datetime.now() - start_time).total_seconds()
                avg_time = elapsed / max(saved_count, 1)
                remaining = len(results) - i + 1
                estimated_time = avg_time * remaining
                print(f"[{i}/{len(results)}] æå–ä¸­... (å·²ä¿å­˜: {saved_count}, é¢„è®¡å‰©ä½™: {int(estimated_time)}ç§’) {url[:50]}...")
                
                try:
                    article = client.æå–æ–‡ç« ä¿¡æ¯(url)
                    
                    # ç«‹å³ä¿å­˜
                    success, result_info = save_article({
                        'index': i,
                        'original': result,
                        'article': article
                    }, i)
                    
                    if success:
                        extracted_articles.append({
                            'index': i,
                            'original': result,
                            'article': article
                        })
                        processed_urls.add(url)
                        saved_count += 1
                        # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦
                        if saved_count % 5 == 0:
                            save_progress()
                        print(f"  âœ“ æå–å¹¶ä¿å­˜æˆåŠŸ: {os.path.basename(result_info)}")
                    else:
                        print(f"  âœ— ä¿å­˜å¤±è´¥: {result_info}")
                        failed_urls.append({'url': url, 'error': f'ä¿å­˜å¤±è´¥: {result_info}'})
                        
                except KeyboardInterrupt:
                    # ç”¨æˆ·ä¸­æ–­
                    raise
                except Exception as e:
                    error_msg = str(e)[:100]
                    print(f"  âœ— æå–å¤±è´¥: {error_msg}")
                    failed_urls.append({'url': url, 'error': error_msg})
                    # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤å°è¯•
                    processed_urls.add(url)
                    save_progress()
            
            # ä¿å­˜æœ€ç»ˆè¿›åº¦
            save_progress()
            
            # åˆ é™¤è¿›åº¦æ–‡ä»¶ï¼ˆä»»åŠ¡å®Œæˆï¼‰
            if os.path.exists(progress_file):
                try:
                    os.remove(progress_file)
                except:
                    pass
            
            # æ˜¾ç¤ºæå–ç»“æœæ‘˜è¦
            print("\n" + "=" * 60)
            print(f"âœ“ æå–å®Œæˆï¼æˆåŠŸ: {len(extracted_articles)}, å¤±è´¥: {len(failed_urls)}")
            print(f"âœ“ å·²ä¿å­˜ {saved_count} ä¸ªç»“æœåˆ° {save_dir}/ ç›®å½•")
            print("=" * 60)
            
            # æ˜¾ç¤ºå‰5ä¸ªç»“æœé¢„è§ˆ
            if extracted_articles:
                print("\nå‰5ä¸ªç»“æœé¢„è§ˆ:")
                for item in extracted_articles[:5]:
                    article = item['article']
                    print(f"\n{item['index']}. {article.get('title', 'æ— æ ‡é¢˜')[:50]}")
                    if article.get('author'):
                        print(f"   ä½œè€…: {article.get('author')}")
                    if article.get('content'):
                        content_len = len(article.get('content', ''))
                        print(f"   æ­£æ–‡: {content_len} å­—ç¬¦")
            
            if failed_urls and len(failed_urls) <= 10:
                print(f"\nâš ï¸  ä»¥ä¸‹ {len(failed_urls)} ä¸ª URL æå–å¤±è´¥:")
                for failed in failed_urls[:10]:
                    print(f"   - {failed['url'][:60]}...")
            elif failed_urls:
                print(f"\nâš ï¸  å…± {len(failed_urls)} ä¸ª URL æå–å¤±è´¥ï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªï¼‰:")
                for failed in failed_urls[:10]:
                    print(f"   - {failed['url'][:60]}...")
            
            if saved_count > 0:
                print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²è‡ªåŠ¨ä¿å­˜åˆ°: {os.path.abspath(save_dir)}")
                print(f"   å…± {saved_count} ä¸ªæ–‡ä»¶")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def get_site_map():
    """è·å–ç½‘ç«™åœ°å›¾"""
    print("\n" + "=" * 60)
    print("ğŸ—ºï¸  è·å–ç½‘ç«™åœ°å›¾")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    url = get_user_input("è¯·è¾“å…¥ç½‘ç«™ URL")
    max_links = int(get_user_input("æœ€å¤šè¿”å›é“¾æ¥æ•°", "20"))
    
    print(f"\næ­£åœ¨è·å–ç½‘ç«™åœ°å›¾: {url}")
    print("è¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        links = client.è·å–ç½‘ç«™åœ°å›¾(url, æœ€å¤§é“¾æ¥æ•°=max_links)
        
        print("\n" + "=" * 60)
        print(f"âœ“ æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
        print("=" * 60)
        
        for i, link in enumerate(links, 1):
            print(f"\n{i}. {link.get('title', 'æ— æ ‡é¢˜')}")
            print(f"   URL: {link.get('url', '')}")
            if link.get('description'):
                print(f"   æè¿°: {link.get('description')}")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def batch_scrape():
    """æ‰¹é‡æŠ“å–"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ")
    print("=" * 60)
    
    if not check_api_key():
        return
    
    print("\nè¯·è¾“å…¥è¦æŠ“å–çš„ URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
    urls = []
    while True:
        url = input(f"URL {len(urls) + 1}: ").strip()
        if not url:
            break
        urls.append(url)
    
    if not urls:
        print("âš ï¸  æœªè¾“å…¥ä»»ä½• URL")
        return
    
    print(f"\nå°†æŠ“å– {len(urls)} ä¸ªç½‘é¡µ")
    print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨å€™...")
    
    try:
        client = FirecrawlClient()
        results = client.æ‰¹é‡æŠ“å–(urls)
        
        print("\n" + "=" * 60)
        print(f"âœ“ æ‰¹é‡æŠ“å–å®Œæˆï¼å…± {len(results)} ä¸ªç»“æœ")
        print("=" * 60)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.æ ‡é¢˜}")
            print(f"   URL: {result.URL}")
            print(f"   å†…å®¹é•¿åº¦: {len(result.å†…å®¹)} å­—ç¬¦")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("â„¹ï¸  ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("""
Firecrawl å‹å¥½å®¢æˆ·ç«¯ä½¿ç”¨è¯´æ˜
============================

1. æŠ“å–å•ä¸ªç½‘é¡µ
   - è¾“å…¥ç½‘é¡µ URL
   - é€‰æ‹©è¾“å‡ºæ ¼å¼ï¼ˆMarkdown/HTMLï¼‰
   - å¯ä»¥ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

2. æå–æ–‡ç« ä¿¡æ¯ â­
   - ä»…æå–ï¼šæ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ­£æ–‡
   - è‡ªåŠ¨å»é™¤æ— å…³ä¿¡æ¯ï¼ˆå¯¼èˆªæ ã€é¡µè„šã€å¹¿å‘Šç­‰ï¼‰
   - é€‚åˆæå–æ–°é—»ã€åšå®¢ç­‰æ–‡ç« å†…å®¹

3. çˆ¬å–æ•´ä¸ªç½‘ç«™
   - è¾“å…¥èµ·å§‹ URL
   - è®¾ç½®æœ€å¤šçˆ¬å–é¡µé¢æ•°
   - å¯ä»¥ä¿å­˜æ‰€æœ‰é¡µé¢

4. æœç´¢ç½‘é¡µ
   - è¾“å…¥æœç´¢å…³é”®è¯
   - è®¾ç½®è¿”å›ç»“æœæ•°é‡
   - é€‰æ‹©å¤„ç†æ–¹å¼ï¼š
     * ä»…æ˜¾ç¤ºé“¾æ¥å’Œæè¿°ï¼ˆå¿«é€Ÿï¼‰
     * æŠ“å–å®Œæ•´å†…å®¹ï¼ˆå¯ä¿å­˜ï¼‰
     * æå–æ–‡ç« ä¿¡æ¯ï¼ˆä»…æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€æ­£æ–‡ï¼‰â­
   - å¯ä»¥ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
   - ä¿å­˜ä½ç½®: search_results/ ç›®å½•

5. è·å–ç½‘ç«™åœ°å›¾
   - è¾“å…¥ç½‘ç«™ URL
   - è·å–ç½‘ç«™æ‰€æœ‰é“¾æ¥
   - æ˜¾ç¤ºé“¾æ¥åˆ—è¡¨

6. æ‰¹é‡æŠ“å–
   - è¾“å…¥å¤šä¸ª URL
   - æ‰¹é‡æŠ“å–æ‰€æœ‰ç½‘é¡µ
   - æ˜¾ç¤ºæ‰€æœ‰ç»“æœ

7. è®¾ç½® API å¯†é’¥
   - è®¾ç½® Firecrawl API å¯†é’¥
   - è·å–å¯†é’¥: https://firecrawl.dev

æ›´å¤šä¿¡æ¯:
- æŸ¥çœ‹ firecrawl_client_README.md
- æŸ¥çœ‹ firecrawl_client_examples.py
""")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
    if not check_venv():
        print("âš ï¸  è­¦å‘Š: æœªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ")
        print("å»ºè®®ä½¿ç”¨: source venv/bin/activate")
        input("\næŒ‰ Enter ç»§ç»­...")
    
    while True:
        clear_screen()
        print_header()
        
        # æ£€æŸ¥ API å¯†é’¥çŠ¶æ€
        api_key = os.getenv("FIRECRAWL_API_KEY")
        if api_key:
            print(f"âœ“ API å¯†é’¥å·²è®¾ç½®: {api_key[:10]}...")
        else:
            print("âš ï¸  API å¯†é’¥æœªè®¾ç½®ï¼ˆæŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨ï¼‰")
        
        print_menu()
        
        choice = input("\nè¯·é€‰æ‹© (0-8): ").strip()
        
        if choice == "0":
            print("\nğŸ‘‹ å†è§ï¼")
            break
        elif choice == "1":
            scrape_single_page()
        elif choice == "2":
            extract_article()
        elif choice == "3":
            crawl_website()
        elif choice == "4":
            search_web()
        elif choice == "5":
            get_site_map()
        elif choice == "6":
            batch_scrape()
        elif choice == "7":
            set_api_key()
        elif choice == "8":
            show_help()
        else:
            print("\nâŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        if choice != "0":
            input("\næŒ‰ Enter è¿”å›ä¸»èœå•...")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç¨‹åºè¢«ä¸­æ–­")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœæ­£åœ¨æå–æ–‡ç« ï¼Œå·²ä¿å­˜çš„ç»“æœä¸ä¼šä¸¢å¤±")
        print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

